services:
  app:
    build:
      context: .
    restart: unless-stopped
    volumes:
      - ./app:/app
      - static-clubs-prod:/vol/web
    environment:
      - PORT=9000
      - WORKER_COUNT=${WORKER_COUNT:-6}
    env_file:
      - .env
    networks:
      - cluster
    healthcheck:
      test: ["CMD-SHELL", "healthcheck.sh"]
      interval: 60s
      timeout: 10s
      retries: 10
      start_period: 60s
      start_interval: 15s

  app_replica:
    build:
      context: .
    restart: unless-stopped
    volumes:
      - ./app:/app
      - static-clubs-prod:/vol/web
    environment:
      - PORT=9000
      - WORKER_COUNT=${WORKER_COUNT:-6}
    env_file:
      - .env
    networks:
      - cluster
    depends_on:
      # Wait until app is ready, in case app needs to run migrations
      app:
        condition: service_healthy
    deploy:
      mode: replicated
      endpoint_mode: vip
      replicas: ${APP_REPLICAS:-3}

  redis:
    image: redis:alpine
    container_name: clubs-prod-redis
    networks:
      - cluster

  celery:
    build:
      context: .
    restart: unless-stopped
    user: django-user
    command: ["celery", "-A", "app", "worker", "--loglevel=info"]
    volumes:
      - ./app:/app
      - static-clubs-prod:/vol/web
    depends_on:
      redis:
        condition: service_started
      app:
        condition: service_healthy
    env_file:
      - .env
    networks:
      - cluster

  celerybeat:
    build:
      context: .
    user: django-user
    restart: unless-stopped
    command:
      [
        "celery",
        "-A",
        "app",
        "beat",
        "--loglevel=info",
        "--scheduler",
        "django_celery_beat.schedulers:DatabaseScheduler",
      ]
    volumes:
      - ./app:/app
      - static-clubs-prod:/vol/web
    depends_on:
      redis:
        condition: service_started
      app:
        condition: service_healthy
      celery:
        condition: service_started

    env_file:
      - .env
    networks:
      - cluster

  proxy:
    build:
      context: ./deploy/proxy/
    restart: unless-stopped
    container_name: clubs-prod-proxy
    ports:
      - 8080:8080
    depends_on:
      # Wait until app is ready, in case app needs to run migrations
      app:
        condition: service_healthy
    volumes:
      - static-clubs-prod:/vol/web
    environment:
      - SERVER_URI=app:9000
      - SERVER_REPLICA_URI=app_replica:9000
      - PROXY_DOCS_URI=${PROXY_DOCS_URI:-''}
      # No delay needed, since app has health checks
      - DELAY=0s
    networks:
      - cluster

  # # USED FOR TESTING ONLY
  # postgres:
  #   image: postgres:15-alpine
  #   command: -c 'max_connections=1000'
  #   volumes:
  #     - postgres-clubs-prod:/var/lib/postgresql/data
  #   environment:
  #     - POSTGRES_DB=devdatabase
  #     - POSTGRES_USER=devuser
  #     - POSTGRES_PASSWORD=devpass
  #   networks:
  #     - cluster

  # pgbouncer:
  #   image: edoburu/pgbouncer
  #   container_name: pgbouncer
  #   restart: always
  #   depends_on:
  #     - postgres
  #   networks:
  #     - cluster
  #   configs:
  #     - source: pgbouncer.ini
  #       target: /etc/pgbouncer/pgbouncer.ini
  #     - source: userlist.txt
  #       target: /etc/pgbouncer/userlist.txt

volumes:
  static-clubs-prod:

  # # USED FOR TESTING ONLY
  # postgres-clubs-prod:

# # USED FOR TESTING ONLY
# # Mimics production database pooling support
# configs:
#   pgbouncer.ini:
#     content: |
#       [databases]
#       * = host=postgres port=5432 auth_user=devuser dbname=devdatabase
#       devdatabase = host=postgres port=5432 auth_user=devuser

#       [pgbouncer]
#       listen_addr = *
#       listen_port = 6432
#       auth_type = plain
#       auth_file = /etc/pgbouncer/userlist.txt
#       pool_mode = transaction
#       max_client_conn = 1000
#       default_pool_size = 100

#       client_idle_timeout = 600 ; 10 minutes
#       server_idle_timeout = 300 ; 5 minutes
#       server_lifetime = 3600 ; 1 hour
#       query_wait_timeout = 120 ; 2 minutes
#       connect_timeout = 5

#       admin_users = devuser
#   userlist.txt:
#     content: |
#       "devuser" "devpass"

networks:
  cluster:
    name: club_portal_prod
    driver: bridge
